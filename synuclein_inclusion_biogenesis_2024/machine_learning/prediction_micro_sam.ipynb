{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a02530c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'czifile'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmicro_sam\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mevaluation\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_comparison\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _enhance_image\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmicro_sam\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mautomatic_segmentation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_predictor_and_segmenter, automatic_instance_segmentation\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mczifile\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m imread\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'czifile'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "from typing import Optional, Union, Tuple\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.measure import label as connected_components\n",
    "\n",
    "import torch\n",
    "\n",
    "from torch_em.util.util import get_random_colors\n",
    "from torch_em.data.datasets.electron_microscopy.lucchi import get_lucchi_paths\n",
    "from torch_em.data.datasets.light_microscopy.covid_if import get_covid_if_data\n",
    "\n",
    "from micro_sam.evaluation.model_comparison import _enhance_image\n",
    "from micro_sam.automatic_segmentation import get_predictor_and_segmenter, automatic_instance_segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8da27cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_automatic_instance_segmentation(\n",
    "    image: np.ndarray,\n",
    "    ndim: int,\n",
    "    checkpoint_path: Optional[Union[os.PathLike, str]] = None,\n",
    "    model_type: str = \"vit_b_lm\",\n",
    "    device: Optional[Union[str, torch.device]] = None,\n",
    "    tile_shape: Optional[Tuple[int, int]] = None,\n",
    "    halo: Optional[Tuple[int, int]] = None,\n",
    "):\n",
    "    \"\"\"Automatic Instance Segmentation (AIS) by training an additional instance decoder in SAM.\n",
    "\n",
    "    NOTE: AIS is supported only for `µsam` models.\n",
    "\n",
    "    Args:\n",
    "        image: The input image.\n",
    "        ndim: The number of dimensions for the input data.\n",
    "        checkpoint_path: The path to stored checkpoints.\n",
    "        model_type: The choice of the `µsam` model.\n",
    "        device: The device to run the model inference.\n",
    "        tile_shape: The tile shape for tiling-based segmentation.\n",
    "        halo: The overlap shape on each side per tile for stitching the segmented tiles.\n",
    "\n",
    "    Returns:\n",
    "        The instance segmentation.\n",
    "    \"\"\"\n",
    "    # Step 1: Get the 'predictor' and 'segmenter' to perform automatic instance segmentation.\n",
    "    predictor, segmenter = get_predictor_and_segmenter(\n",
    "        model_type=model_type,  # choice of the Segment Anything model\n",
    "        checkpoint=checkpoint_path,  # overwrite to pass your own finetuned model.\n",
    "        device=device,  # the device to run the model inference.\n",
    "        amg=False,  # set the automatic segmentation mode to AIS.\n",
    "        is_tiled=(tile_shape is not None),  # whether to run automatic segmentation with tiling.\n",
    "    )\n",
    "\n",
    "    # Step 2: Get the instance segmentation for the given image.\n",
    "    prediction = automatic_instance_segmentation(\n",
    "        predictor=predictor,  # the predictor for the Segment Anything model.\n",
    "        segmenter=segmenter,  # the segmenter class responsible for generating predictions.\n",
    "        input_path=image,  # the filepath to image or the input array for automatic segmentation.\n",
    "        ndim=ndim,  # the number of input dimensions.\n",
    "        tile_shape=tile_shape,  # the tile shape for tiling-based prediction.\n",
    "        halo=halo,  # the overlap shape for tiling-based prediction.\n",
    "    )\n",
    "\n",
    "    return prediction\n",
    "\n",
    "\n",
    "def run_automatic_mask_generation(\n",
    "    image: np.ndarray,\n",
    "    ndim: int,\n",
    "    checkpoint_path: Optional[Union[os.PathLike, str]] = None,\n",
    "    model_type: str = \"vit_b\",\n",
    "    device: Optional[Union[str, torch.device]] = None,\n",
    "    tile_shape: Optional[Tuple[int, int]] = None,\n",
    "    halo: Optional[Tuple[int, int]] = None,\n",
    "):\n",
    "    \"\"\"Automatic Mask Generation (AMG) is the automatic segmentation method offered by SAM.\n",
    "\n",
    "    NOTE: AMG is supported for both Segment Anything models and `µsam` models.\n",
    "\n",
    "    Args:\n",
    "        image: The input image.\n",
    "        ndim: The number of dimensions for the input data.\n",
    "        checkpoint_path: The path to stored checkpoints.\n",
    "        model_type: The choice of the SAM / `µsam` model.\n",
    "        device: The device to run the model inference.\n",
    "        tile_shape: The tile shape for tiling-based segmentation.\n",
    "        halo: The overlap shape on each side per tile for stitching the segmented tiles.\n",
    "\n",
    "    Returns:\n",
    "        The instance segmentation.\n",
    "    \"\"\"\n",
    "    # Step 1: Get the 'predictor' and 'segmenter' to perform automatic instance segmentation.\n",
    "    predictor, segmenter = get_predictor_and_segmenter(\n",
    "        model_type=model_type,  # choice of the Segment Anything model\n",
    "        checkpoint=checkpoint_path,  # overwrite to pass your own finetuned model.\n",
    "        device=device,  # the device to run the model inference.\n",
    "        amg=True,  # set the automatic segmentation mode to AMG.\n",
    "        is_tiled=(tile_shape is not None),  # whether to run automatic segmentation with tiling.\n",
    "    )\n",
    "\n",
    "    # Step 2: Get the instance segmentation for the given image.\n",
    "    prediction = automatic_instance_segmentation(\n",
    "        predictor=predictor,  # the predictor for the Segment Anything model.\n",
    "        segmenter=segmenter,  # the segmenter class responsible for generating predictions.\n",
    "        input_path=image,  # the filepath to image or the input array for automatic segmentation.\n",
    "        ndim=ndim,  # the number of input dimensions.\n",
    "        tile_shape=tile_shape,  # the tile shape for tiling-based prediction.\n",
    "        halo=halo,  # the overlap shape for tiling-based prediction.\n",
    "    )\n",
    "\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0edc4861",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_two_images(image1, image2, title1, title2, path):\n",
    "    \"\"\"Display two images side-by-side with smaller title font.\"\"\"\n",
    "    filename = os.path.basename(path)  # Extract final part of path\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "    axes[0].imshow(image1, cmap='gray' if image1.ndim == 2 else None)\n",
    "    axes[0].set_title(f\"{filename} {title1}\", fontsize=10)\n",
    "    axes[0].axis('off')\n",
    "\n",
    "    axes[1].imshow(image2, cmap='gray' if image2.ndim == 2 else None)\n",
    "    axes[1].set_title(f\"{filename} {title2}\", fontsize=10)\n",
    "    axes[1].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def display_image(image, path, type):\n",
    "    \"\"\"Display the image.\"\"\"\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.title(f\"{path} {type}\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def extract_image_paths(folder):\n",
    "    \"\"\"Extract all image file paths from the specified folder.\"\"\"\n",
    "    return [os.path.join(folder, f) for f in os.listdir(folder) if os.path.isfile(os.path.join(folder, f))]\n",
    "\n",
    "def read_image(image_path):\n",
    "    \"\"\"Read the LSM image from the specified path.\"\"\"\n",
    "    return imread(image_path)\n",
    "\n",
    "def extract_channels(image: np.ndarray) -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Extract green and red channels from the squeezed image (shape: [Z, C, H, W]).\"\"\" \n",
    "    return image[0], image[1], image[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9fc8abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_choice = \"vit_b_em_organelles\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "micro-sam-annotation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

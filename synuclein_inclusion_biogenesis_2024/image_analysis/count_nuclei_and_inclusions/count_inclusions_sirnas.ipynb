{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48a71267",
   "metadata": {},
   "source": [
    "This code is optimized to count inclusions and nuclei for images with transfections and cotransfections."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18988fc9",
   "metadata": {},
   "source": [
    "Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24eb7961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.io import imread\n",
    "from skimage.filters import gaussian, threshold_otsu\n",
    "from skimage import morphology, exposure\n",
    "from skimage.measure import label, regionprops\n",
    "from skimage.morphology import remove_small_objects, binary_dilation, disk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9664f8",
   "metadata": {},
   "source": [
    "Define Sub Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1b3a312",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract image file paths from a specified folder\n",
    "def extract_image_paths(folder_path):\n",
    "    return [os.path.join(folder_path, f) for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))]\n",
    "\n",
    "# Function to preprocess and segment the DAPI channel to count nuclei\n",
    "def count_nuclei(dapi_channel):\n",
    "    # Apply Gaussian blur to smooth out noise\n",
    "    blurred_dapi = gaussian(dapi_channel, sigma=2)\n",
    "    \n",
    "    # Thresholding to segment nuclei\n",
    "    threshold_value = threshold_otsu(blurred_dapi)\n",
    "    binary_image = blurred_dapi > threshold_value\n",
    "    \n",
    "    # Morphological operations to clean up the segmentation\n",
    "    filled_image = morphology.remove_small_holes(binary_image, area_threshold=50000)\n",
    "    cleaned_image = remove_small_objects(filled_image, min_size=400)\n",
    "    \n",
    "    # Dilate to merge adjacent objects\n",
    "    selem = disk(5)\n",
    "    merged_image = binary_dilation(cleaned_image, footprint=selem)\n",
    "    \n",
    "    # Label the objects and count the nuclei\n",
    "    labeled_image = label(merged_image)\n",
    "    n_objects = len(np.unique(labeled_image)) - 1  # Exclude background label (0)\n",
    "    \n",
    "    return n_objects, labeled_image\n",
    "\n",
    "# Function to preprocess and segment the green channel to count inclusions\n",
    "def count_inclusions(green_channel):\n",
    "    # Adjust brightness and contrast\n",
    "    adjusted_img = exposure.adjust_sigmoid(green_channel, cutoff=0.4)\n",
    "    \n",
    "    # Normalize the pixel values\n",
    "    normalized_img = (adjusted_img - adjusted_img.min()) / (adjusted_img.max() - adjusted_img.min())\n",
    "    \n",
    "    # Thresholding to segment inclusions\n",
    "    threshold_value = 0.3  # Adjust as needed\n",
    "    binary_image = normalized_img > threshold_value\n",
    "    \n",
    "    # Label the inclusions and measure their sizes\n",
    "    labeled_image = label(binary_image)\n",
    "    props = regionprops(labeled_image, normalized_img)\n",
    "    \n",
    "    sizes = [prop.area for prop in props]\n",
    "    \n",
    "    # Filter out very small inclusions\n",
    "    filtered_sizes = [size for size in sizes if size > 10]\n",
    "    \n",
    "    return filtered_sizes, labeled_image\n",
    "\n",
    "# Function to analyze all images in a folder\n",
    "def analyze_images(folder_path):\n",
    "    # Extract all image file paths\n",
    "    images_to_analyze = extract_image_paths(folder_path)\n",
    "    \n",
    "    \n",
    "    # Initialize lists to store results\n",
    "    number_of_nuclei_list = []\n",
    "    mean_sizes_of_inclusions = []\n",
    "    \n",
    "    sizes_df_new = pd.DataFrame()\n",
    "    \n",
    "    # Iterate through each image\n",
    "    for iteration, image_path in enumerate(images_to_analyze, start=1):\n",
    "        # Read the image\n",
    "        image = imread(image_path)\n",
    "        \n",
    "        # Extract channels\n",
    "        dapi_channel = image[0]  # DAPI channel\n",
    "        green_channel = image[1]  # Green channel\n",
    "        \n",
    "        # Count nuclei\n",
    "        n_nuclei, labeled_nuclei_image = count_nuclei(dapi_channel)\n",
    "        number_of_nuclei_list.append(n_nuclei)\n",
    "        \n",
    "        # Count inclusions\n",
    "        inclusion_sizes, labeled_inclusion_image = count_inclusions(green_channel)\n",
    "        mean_sizes_of_inclusions.append(np.mean(inclusion_sizes))\n",
    "        \n",
    "        # Create DataFrame for current image's inclusion sizes\n",
    "        sizes_df_add = pd.DataFrame(inclusion_sizes, columns=[f'image {iteration}'])\n",
    "        \n",
    "        # Concatenate to the main DataFrame\n",
    "        sizes_df_new = pd.concat([sizes_df_new, sizes_df_add], axis=1)\n",
    "    \n",
    "    return sizes_df_new, number_of_nuclei_list, mean_sizes_of_inclusions, images_to_analyze\n",
    "\n",
    "# Function to finalize and save the results to Excel files\n",
    "def save_results(sizes_df_new, number_of_nuclei_list, mean_sizes_of_inclusions, images_to_analyze):\n",
    "    basenames = [os.path.basename(path) for path in images_to_analyze]\n",
    "    # Replace all values of 1 with NaN and drop NaN values to the bottom\n",
    "    sizes_df_new = sizes_df_new.replace(1.0, np.NaN)\n",
    "    sizes_df_new = sizes_df_new.apply(lambda x: pd.Series(x.dropna().values))\n",
    "    \n",
    "    # Transpose for nuclei count\n",
    "    sizes_df_new_nuclei = sizes_df_new.transpose()\n",
    "    \n",
    "    # Count total number of inclusions per image\n",
    "    number_of_inclusions = sizes_df_new_nuclei.count(axis=1)\n",
    "    \n",
    "    # Create DataFrame for the second Excel file\n",
    "    excel_2 = pd.DataFrame({\n",
    "        'Image_Name': basenames,\n",
    "        'Number_of_Inclusions': number_of_inclusions,\n",
    "        'Number_of_Nuclei': number_of_nuclei_list,\n",
    "        'Average_Number_of_Inclusions_per_Cell': number_of_inclusions / number_of_nuclei_list,\n",
    "        'Mean_Sizes_Of_Inclusions': mean_sizes_of_inclusions\n",
    "    })\n",
    "    \n",
    "    # Replace NaN values with empty strings\n",
    "    sizes_df_new = sizes_df_new.fillna('')\n",
    "    \n",
    "    # Save DataFrames to Excel files\n",
    "    # sizes_df_new.to_excel(\"output.xlsx\")\n",
    "    excel_2.to_excel(\"SUMMARY.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28384eb8",
   "metadata": {},
   "source": [
    "Define Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46420589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function to run the analysis\n",
    "def main():\n",
    "    folder_path = \"test_images_sirnas\"\n",
    "    sizes_df_new, number_of_nuclei_list, mean_sizes_of_inclusions, images_to_analyze = analyze_images(folder_path)\n",
    "    save_results(sizes_df_new, number_of_nuclei_list, mean_sizes_of_inclusions, images_to_analyze)\n",
    "\n",
    "# Run the main function\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

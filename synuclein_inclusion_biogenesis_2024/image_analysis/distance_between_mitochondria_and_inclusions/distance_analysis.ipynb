{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9706e858",
   "metadata": {},
   "source": [
    "C:\\Users\\yh1024\\Box\\LAB\\Lab Folder\\EXPERIMENTS\\CONFOCAL EXPERIMENTS\\Mito_OA_1K_wt_3K\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d30d31b",
   "metadata": {},
   "source": [
    "Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a15984f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yh1024\\Documents\\Yiming Huang\\.venv\\Lib\\site-packages\\cellpose\\resnet_torch.py:275: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(filename, map_location=torch.device(\"cpu\"))\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.filters import gaussian, threshold_otsu, threshold_multiotsu\n",
    "from skimage.measure import label, regionprops\n",
    "from skimage.morphology import remove_small_objects, binary_dilation, disk\n",
    "import czifile\n",
    "import skimage.io as io\n",
    "from skimage import exposure\n",
    "from cellpose import models\n",
    "from skimage.filters import unsharp_mask\n",
    "from skimage.segmentation import find_boundaries\n",
    "\n",
    "model = models.Cellpose(model_type='cyto3')\n",
    "# model = models.Cellpose(model_type='nuclei')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "118e8cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_INCLUSION_SIZE = 10\n",
    "MAX_INCLUSION_SIZE = 10000\n",
    "COLOR_SCHEME = 'hot'\n",
    "RADIUS = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43beac27",
   "metadata": {},
   "source": [
    "Define Sub Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4df11f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(image,cmap=COLOR_SCHEME):\n",
    "    io.imshow(image, cmap=cmap)\n",
    "    plt.show()\n",
    "\n",
    "def preprocess_green_channel(green_channel):\n",
    "    \"\"\"Preprocess the green channel for cell segmentation and inclusion quantification.\"\"\"\n",
    "    confocal_img = gaussian(green_channel, sigma=2)\n",
    "    # increase image contrast\n",
    "    \n",
    "    # confocal_img = exposure.adjust_sigmoid(green_channel, cutoff=0.25)\n",
    "    confocal_img = (confocal_img - confocal_img.min()) / (confocal_img.max() - confocal_img.min())\n",
    "    \n",
    "    return confocal_img\n",
    "\n",
    "def preprocess_dapi_channel(dapi_channel):\n",
    "    \"\"\"Preprocess the DAPI channel for nuclei quantification.\"\"\"\n",
    "    blurred_dapi = gaussian(dapi_channel, sigma=2)\n",
    "    threshold_value = threshold_otsu(blurred_dapi)\n",
    "    binary_image = blurred_dapi > threshold_value\n",
    "    # show_image(binary_image)\n",
    "    cleaned_image = remove_small_objects(binary_image, min_size=400)\n",
    "    merged_image = binary_dilation(cleaned_image, footprint=disk(5))\n",
    "    labeled_image = label(merged_image)\n",
    "    return labeled_image\n",
    "\n",
    "def count_nuclei(labeled_image):\n",
    "    \"\"\"Count the number of nuclei in the labeled image.\"\"\"\n",
    "    return len(np.unique(labeled_image)) - 1\n",
    "\n",
    "\n",
    "def increase_contrast(image):\n",
    "    \"\"\"Increase the contrast of the image.\"\"\"\n",
    "    p2, p98 = np.percentile(image, (2, 98))\n",
    "    return exposure.rescale_intensity(image, in_range=(p2, p98))\n",
    "\n",
    "def segment_cells(green_channel):\n",
    "    \"\"\"Segment cells in the green channel.\"\"\"\n",
    "    mean_intensity = np.mean(green_channel[green_channel > 0.2])\n",
    "    # print(\"Mean Intensity: \", mean_intensity)\n",
    "    green_channel_remove_inclusions = np.where(green_channel < 0.6, green_channel, mean_intensity) # dim down inclusions\n",
    "    green_channel_remove_inclusions = gaussian(green_channel_remove_inclusions, sigma=5) # blur\n",
    "    \n",
    "    # normalize\n",
    "    green_channel_remove_inclusions = (green_channel_remove_inclusions - np.min(green_channel_remove_inclusions)) / ((np.max(green_channel_remove_inclusions) - np.min(green_channel_remove_inclusions)))\n",
    "    # show_image(green_channel_remove_inclusions)\n",
    "\n",
    "    # Detect cells in the green channel\n",
    "    # keep increasing diameter until cells are detected\n",
    "    diameter = 150\n",
    "\n",
    "    while diameter < 500:\n",
    "\n",
    "        masks, flows, styles, diams = model.eval(green_channel_remove_inclusions, diameter=diameter, channels=[0, 0])\n",
    "        labeled_cells = label(masks)\n",
    "        # if there are more than 0 cells\n",
    "        if np.max(labeled_cells) > 0:\n",
    "            return labeled_cells\n",
    "        diameter += 25\n",
    "    # no cells found, throw an error\n",
    "    return None\n",
    "\n",
    "def extract_inclusions(green_channel,mask,display_graph=False):\n",
    "    applied_mask_blurred = gaussian(green_channel, sigma=1) * mask\n",
    "    applied_mask_eliminate_background = applied_mask_blurred[applied_mask_blurred > 0] # filter out 0\n",
    "    \n",
    "    # normalize \n",
    "    applied_mask_eliminate_background = (applied_mask_eliminate_background - np.min(applied_mask_eliminate_background)) / ((np.max(applied_mask_eliminate_background) - np.min(applied_mask_eliminate_background)))\n",
    "\n",
    "    # calculate statistics\n",
    "    mean_intensity = np.mean(applied_mask_eliminate_background)\n",
    "    median_intensity = np.median(applied_mask_eliminate_background)\n",
    "    q1 = np.percentile(applied_mask_eliminate_background, 25)\n",
    "    q3 = np.percentile(applied_mask_eliminate_background, 75)\n",
    "    \n",
    "    hist, bin_edges = np.histogram(applied_mask_eliminate_background, bins='fd')\n",
    "    \n",
    "    applied_mask = green_channel * mask\n",
    "    # 0.5 is an empirical value that seems to work well\n",
    "    if q3 < 0.5 and len(bin_edges) > 20:\n",
    "        # the population is skewed to the left, indicating that there are inclusions\n",
    "        threshold = max(threshold_otsu(applied_mask),0.5) # use otsu thresholding to find the threshold\n",
    "    elif q3 >= 0.7:\n",
    "        # definitely no inclusions\n",
    "        threshold = 1\n",
    "    else:\n",
    "        # probably no inclusions, but not sure, so we set a very high threshold in case there are inclusions\n",
    "        threshold = 0.999\n",
    "    \n",
    "    inclusions = applied_mask > threshold # apply the threshold\n",
    "\n",
    "    inclusions = remove_small_objects(inclusions, min_size=MIN_INCLUSION_SIZE) # remove small objects\n",
    "    inclusions = inclusions ^ remove_small_objects(inclusions, min_size=MAX_INCLUSION_SIZE) # remove large objects\n",
    "\n",
    "    if display_graph:\n",
    "        \n",
    "        print(\"Threshold: \", threshold)\n",
    "        print(\"Bin count\", len(bin_edges))\n",
    "\n",
    "        plt.hist(applied_mask_eliminate_background, bins='fd')  \n",
    "        plt.axvline(mean_intensity, color='red', linestyle='dashed', linewidth=2, label=f'Mean: {mean_intensity:.2f}')\n",
    "        plt.axvline(median_intensity, color='green', linestyle='dashed', linewidth=2, label=f'Median: {median_intensity:.2f}')\n",
    "        plt.axvline(q1, color='blue', linestyle='dashed', linewidth=2, label=f'Q1: {q1:.2f}')\n",
    "        plt.axvline(q3, color='purple', linestyle='dashed', linewidth=2, label=f'Q3: {q3:.2f}')\n",
    "        plt.legend()\n",
    "        plt.title(\"Intensity histogram\")\n",
    "        plt.show()\n",
    "\n",
    "    return inclusions\n",
    "\n",
    "\n",
    "def sliding_window_density_same_size(binary_image, window_size=(10, 10), stride=(5, 5)):\n",
    "    \"\"\"\n",
    "    Calculate the density map for a binary image using a sliding window approach, \n",
    "    ensuring the output map is the same size as the input image.\n",
    "    \n",
    "    Args:\n",
    "    binary_image (ndarray): Binary image (values 0 and 1).\n",
    "    window_size (tuple): Size of the sliding window (height, width).\n",
    "    stride (tuple): Step size for the sliding window (vertical, horizontal).\n",
    "    \n",
    "    Returns:\n",
    "    density_map (ndarray): Density map of the same shape as the original image.\n",
    "    \"\"\"\n",
    "    h, w = binary_image.shape\n",
    "    win_h, win_w = window_size\n",
    "    stride_h, stride_w = stride\n",
    "    \n",
    "    # Initialize arrays for density accumulation and counts\n",
    "    density_accumulation = np.zeros_like(binary_image, dtype=np.float32)\n",
    "    count_accumulation = np.zeros_like(binary_image, dtype=np.float32)\n",
    "\n",
    "    # Slide the window across the image\n",
    "    for y in range(0, h - win_h + 1, stride_h):\n",
    "        for x in range(0, w - win_w + 1, stride_w):\n",
    "            # Extract the window\n",
    "            window = binary_image[y:y + win_h, x:x + win_w]\n",
    "            \n",
    "            # Calculate density (percentage of white pixels)\n",
    "            density = np.sum(window) / (win_h * win_w)\n",
    "            \n",
    "            # Add density to the corresponding region\n",
    "            density_accumulation[y:y + win_h, x:x + win_w] += density\n",
    "            \n",
    "            # Count how many times each pixel is visited\n",
    "            count_accumulation[y:y + win_h, x:x + win_w] += 1\n",
    "\n",
    "    # Avoid division by zero and calculate the final density map\n",
    "    density_map = np.divide(\n",
    "        density_accumulation, \n",
    "        count_accumulation, \n",
    "        out=np.zeros_like(density_accumulation), \n",
    "        where=(count_accumulation != 0)\n",
    "    )\n",
    "\n",
    "    return density_map\n",
    "\n",
    "def show_density_map_with_contour(green_channel, orange_channel_thresholded, labeled_cells,hasOA, verbose=False):\n",
    "    \"\"\"Show the density map with cell contours.\"\"\"\n",
    "    density_map = sliding_window_density_same_size(orange_channel_thresholded, window_size=(10, 10), stride=(1, 1))\n",
    "    # normalize to 0 - 0.5\n",
    "    # density_map = density_map / np.max(density_map) * 0.5\n",
    "    mask = labeled_cells > 0\n",
    "    \n",
    "    # print(\"Segmented Cells\")\n",
    "    # show_image(labeled_cells)\n",
    "\n",
    "\n",
    "    # oshow the contour of the cells\n",
    "\n",
    "    labeled_cells_contours = find_boundaries(labeled_cells, mode='outer')\n",
    "    density_map_inside_cells = density_map * mask\n",
    "\n",
    "    # combine cell contour and density_map_inside_cells into one image\n",
    "    combined_image = density_map_inside_cells \n",
    "    combined_image[labeled_cells_contours] = 1\n",
    "    # show_image(combined_image,cmap=COLOR_SCHEME)\n",
    "    inclusion_image = generate_inclusion_image(green_channel, labeled_cells, hasOA)\n",
    "    # show_image(inclusion_image,cmap=COLOR_SCHEME)\n",
    "    # print(\"Inclusion Image\")\n",
    "    # show_image(inclusion_image)\n",
    "    mask2 = (inclusion_image > 0) * (combined_image <= 0.1)\n",
    "    combined_image[mask2] = 1\n",
    "    # combined_image[inclusion_image > 0 ] = 1\n",
    "    if verbose:\n",
    "        print(\"Mitochondria Density Map\")\n",
    "        show_image(combined_image,cmap=COLOR_SCHEME)\n",
    "\n",
    "def remove_overlapping_lipids(red_channel_thresholded,inclusion_image):\n",
    "    # use regionprops to separate the lipids\n",
    "    # make result the same size as red_channel_thresholded\n",
    "    result = np.zeros_like(red_channel_thresholded)\n",
    "    labeled_red_channel = label(red_channel_thresholded)\n",
    "    for i, lipid in enumerate(regionprops(labeled_red_channel)):\n",
    "        lipid_image = red_channel_thresholded * (labeled_red_channel == lipid.label)\n",
    "        overlap = lipid_image * inclusion_image\n",
    "        if np.sum(overlap) > 0:\n",
    "            continue\n",
    "        result[lipid.coords[:, 0], lipid.coords[:, 1]] = 1\n",
    "    return result\n",
    "\n",
    "def generate_inclusion_image(green_channel, labeled_cells, hasOA):\n",
    "    if not hasOA:\n",
    "        return np.zeros_like(green_channel)\n",
    "    inclusion_image = np.zeros_like(green_channel)\n",
    "\n",
    "    for i, cell in enumerate(regionprops(labeled_cells)):\n",
    "        if cell.area < 100:\n",
    "            continue\n",
    "        mask = labeled_cells == cell.label\n",
    "        inclusions = extract_inclusions(green_channel, mask)\n",
    "        inclusion_image += inclusions\n",
    "    # show_image(inclusion_image)\n",
    "    return inclusion_image\n",
    "\n",
    "def find_swiss_cheese_inclusions(inclusion_image,red_channel_thresholded, verbose=False):\n",
    "    swiss_chess_inclusions = np.zeros_like(inclusion_image)\n",
    "    regular_inclusions = np.zeros_like(inclusion_image)\n",
    "    labeled_inclusions = label(inclusion_image)\n",
    "    for i, inclusion in enumerate(regionprops(labeled_inclusions)):\n",
    "        mask = labeled_inclusions == inclusion.label\n",
    "        overlap = mask * red_channel_thresholded\n",
    "        if np.sum(overlap) > 0:\n",
    "            swiss_chess_inclusions += mask\n",
    "        else:\n",
    "            regular_inclusions += mask\n",
    "    return swiss_chess_inclusions, regular_inclusions\n",
    "def calculate_densities(green_channel, orange_channel_thresholded, labeled_cells,red_channel_thresholded,hasOA, verbose=False):\n",
    "    inclusion_image = generate_inclusion_image(green_channel, labeled_cells,hasOA)\n",
    "    mask = labeled_cells > 0\n",
    "    # dilate inclusion_image\n",
    "    dilated_inclusion_image = binary_dilation(inclusion_image, disk(RADIUS))\n",
    "    dilated_inclusion_image = dilated_inclusion_image * mask\n",
    "    overlap_with_inclusions = orange_channel_thresholded * dilated_inclusion_image\n",
    "    density_around_inclusions = np.sum(overlap_with_inclusions) / np.sum(dilated_inclusion_image)\n",
    "\n",
    "    # subtract inclusion image from red_channel_thresholded\n",
    "    red_channel_thresholded_remove_overlap = remove_overlapping_lipids(red_channel_thresholded, inclusion_image)\n",
    "    dilated_red_channel_thresholded = binary_dilation(red_channel_thresholded_remove_overlap*mask, disk(RADIUS))\n",
    "    dilated_red_channel_thresholded = dilated_red_channel_thresholded * mask\n",
    "    overlap_with_lipids = orange_channel_thresholded * dilated_red_channel_thresholded\n",
    "    density_around_lipids = np.sum(overlap_with_lipids) / np.sum(dilated_red_channel_thresholded)\n",
    "\n",
    "    rest_of_the_cells = mask ^ (dilated_inclusion_image | dilated_red_channel_thresholded)\n",
    "    overlap_with_rest_of_the_cells = orange_channel_thresholded * rest_of_the_cells\n",
    "    density_around_rest_of_the_cells = np.sum(overlap_with_rest_of_the_cells) / np.sum(rest_of_the_cells)\n",
    "\n",
    "    # swiss cheese and regular inclusions\n",
    "    swiss_chess_inclusions, regular_inclusions = find_swiss_cheese_inclusions(inclusion_image, red_channel_thresholded, verbose=verbose)\n",
    "    dilated_swiss_chess_inclusions = binary_dilation(swiss_chess_inclusions, disk(RADIUS))\n",
    "    dilated_swiss_chess_inclusions = dilated_swiss_chess_inclusions * mask\n",
    "    overlap_with_swiss_chess_inclusions = orange_channel_thresholded * dilated_swiss_chess_inclusions\n",
    "    density_around_swiss_chess_inclusions = np.sum(overlap_with_swiss_chess_inclusions) / np.sum(dilated_swiss_chess_inclusions)\n",
    "\n",
    "    dilated_regular_inclusions = binary_dilation(regular_inclusions, disk(RADIUS))\n",
    "    dilated_regular_inclusions = dilated_regular_inclusions * mask\n",
    "    overlap_with_regular_inclusions = orange_channel_thresholded * dilated_regular_inclusions\n",
    "    density_around_regular_inclusions = np.sum(overlap_with_regular_inclusions) / np.sum(dilated_regular_inclusions)\n",
    "\n",
    "\n",
    "        \n",
    "    if verbose:\n",
    "        inclusion_image_combined = dilated_inclusion_image.copy().astype(np.uint8) # convert to int\n",
    "        inclusion_image_combined[inclusion_image > 0] = 2\n",
    "        inclusion_image_combined = inclusion_image_combined * mask\n",
    "        print(\"Inclusions Dilated\")\n",
    "        show_image(inclusion_image_combined, cmap=COLOR_SCHEME)\n",
    "        \n",
    "        \n",
    "        red_channel_combined = dilated_red_channel_thresholded.copy().astype(np.uint8) # convert to int\n",
    "        red_channel_combined[red_channel_thresholded_remove_overlap > 0] = 2\n",
    "        red_channel_combined = red_channel_combined * mask\n",
    "        print(\"Lipids Dilated (Exclude lipids in Swiss Cheese Inclusions)\")\n",
    "        show_image(red_channel_combined, cmap=COLOR_SCHEME)\n",
    "\n",
    "\n",
    "        print(\"Rest of the cells\")\n",
    "        show_image(rest_of_the_cells, cmap=COLOR_SCHEME)\n",
    "\n",
    "        print(\"Swiss Cheese Inclusions\")\n",
    "        show_image(swiss_chess_inclusions, cmap=COLOR_SCHEME)\n",
    "\n",
    "        print(\"Regular Inclusions\")\n",
    "        show_image(regular_inclusions, cmap=COLOR_SCHEME)\n",
    "    \n",
    "\n",
    "    return density_around_inclusions, density_around_lipids, density_around_rest_of_the_cells, density_around_swiss_chess_inclusions, density_around_regular_inclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966def43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_image(image_path, basename,verbose=False):\n",
    "\n",
    "    \n",
    "    # Load the image and squeeze dimensions\n",
    "    image = czifile.imread(image_path)\n",
    "    image_squeezed = np.squeeze(image)\n",
    "\n",
    "    # Separate the channels\n",
    "    red_channel = image_squeezed[0, :, :]\n",
    "    orange_channel = image_squeezed[1, :, :]\n",
    "    green_channel = image_squeezed[2, :, :]\n",
    "\n",
    "    threshold_red = threshold_otsu(gaussian(red_channel)) + 0.1\n",
    "    red_channel_thresholded = gaussian(red_channel) > threshold_red\n",
    "\n",
    "\n",
    "    # print(\"Red Channel\")\n",
    "    # show_image(red_channel_thresholded)\n",
    "\n",
    "    green_channel = preprocess_green_channel(green_channel)\n",
    "    if verbose:\n",
    "        print(\"Green Channel\")\n",
    "        show_image(green_channel,'gray')\n",
    "\n",
    "        print(\"Red Channel\")\n",
    "        show_image(red_channel_thresholded,'gray')\n",
    "\n",
    "    orange_channel_thresholded = gaussian(orange_channel) > threshold_otsu(gaussian(orange_channel)) + 0.1\n",
    "    # print(\"Orange Channel\")\n",
    "    # display image and use a cmap where red is the highest intensity, and blue is the lowest\n",
    "\n",
    "\n",
    "    # show_image(orange_channel_thresholded)\n",
    "    # display a density map of the orange channel\n",
    "    \n",
    "\n",
    "    \n",
    "    labeled_cells = segment_cells(green_channel)\n",
    "    if labeled_cells is None:\n",
    "        return None\n",
    "    \n",
    "    hasOA = '_OA_' in basename\n",
    "    show_density_map_with_contour(green_channel, orange_channel_thresholded, labeled_cells,hasOA, verbose)\n",
    "    \n",
    "    density_around_inclusions, density_around_lipids, density_around_rest_of_the_cells, density_around_swiss_chess_inclusions, density_around_regular_inclusions = calculate_densities(green_channel, orange_channel_thresholded, labeled_cells,red_channel_thresholded,hasOA,verbose)\n",
    "\n",
    "\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Density around inclusions: \", density_around_inclusions)\n",
    "        print(\"Density around lipids: \", density_around_lipids)\n",
    "        print(\"Density around rest of the cells: \", density_around_rest_of_the_cells)\n",
    " \n",
    "\n",
    "    df1 = pd.DataFrame({\n",
    "        \"File_Name:\": [basename],\n",
    "        \"Density_Around_All_Inclusions\": [density_around_inclusions],  \n",
    "        \"Density_Around_Swiss_Cheese_Inclusions\": [density_around_swiss_chess_inclusions],\n",
    "        \"Density_Around_Regular_Inclusions\": [density_around_regular_inclusions],\n",
    "        \"Density_Around_Lipids\": [density_around_lipids],\n",
    "        \"Density_Around_Rest_of_the_Cells\": [density_around_rest_of_the_cells]\n",
    "    })\n",
    "    return df1\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "112fe69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_all_images(image_folder, images=None,verbose=False):\n",
    "    print(\"Analyzing images in folder: \", image_folder)\n",
    "    # images is the list of images to analyze, if set to None, analyze all images\n",
    "    all_data_1 = []\n",
    "    for well_image in os.listdir(image_folder):\n",
    "        \n",
    "        if well_image.lower().endswith(\".czi\"):  # Filter for CZI files\n",
    "            if images is not None and well_image not in images:\n",
    "                continue\n",
    "           \n",
    "            # mitochondria distance\\010725_mito_OA\\3K_no_OA_settings\\3K_Lipidtox_01.czi\n",
    "            well_image_path = os.path.join(image_folder, well_image)\n",
    "\n",
    "            well_image_base_name = os.path.basename(well_image)[:-4]\n",
    "\n",
    "            \n",
    "           \n",
    "\n",
    "            if verbose:\n",
    "                print(\"-\" * 200)\n",
    "                print(\"Image: \",well_image_path)\n",
    "                \n",
    "            df1 = analyze_image(well_image_path, well_image_base_name,verbose=verbose)\n",
    "            \n",
    "            if(df1 is not None):\n",
    "                all_data_1.append(df1)\n",
    "            \n",
    "            if verbose:\n",
    "                print(\"-\" * 200)\n",
    "    if len(all_data_1) == 0:\n",
    "        return\n",
    "    combined_df_1 = pd.concat(all_data_1, ignore_index=True)\n",
    "\n",
    "    combined_df_1.to_excel(f\"{image_folder}_analysis_by_image_NEW.xlsx\", index=False)\n",
    "\n",
    "def analyze_subfolders(folder,verbose=False):\n",
    "    for subfolder in os.listdir(folder):\n",
    "        if os.path.isdir(os.path.join(folder, subfolder)):\n",
    "            analyze_all_images(os.path.join(folder, subfolder),verbose=verbose)\n",
    "\n",
    "\n",
    "def analyze_main_folder(folder,verbose=False):\n",
    "    for subfolder in os.listdir(folder):\n",
    "        if os.path.isdir(os.path.join(folder, subfolder)):\n",
    "            analyze_subfolders(os.path.join(folder, subfolder),verbose=verbose)\n",
    "    print(\"Analysis Complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58fcf244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing images in folder:  ./010725_mito_OA\\1K new settings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yh1024\\AppData\\Local\\Temp\\ipykernel_11812\\181764975.py:234: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  density_around_inclusions = np.sum(overlap_with_inclusions) / np.sum(dilated_inclusion_image)\n",
      "C:\\Users\\yh1024\\AppData\\Local\\Temp\\ipykernel_11812\\181764975.py:252: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  density_around_swiss_chess_inclusions = np.sum(overlap_with_swiss_chess_inclusions) / np.sum(dilated_swiss_chess_inclusions)\n",
      "C:\\Users\\yh1024\\AppData\\Local\\Temp\\ipykernel_11812\\181764975.py:257: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  density_around_regular_inclusions = np.sum(overlap_with_regular_inclusions) / np.sum(dilated_regular_inclusions)\n",
      "C:\\Users\\yh1024\\AppData\\Local\\Temp\\ipykernel_11812\\181764975.py:241: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  density_around_lipids = np.sum(overlap_with_lipids) / np.sum(dilated_red_channel_thresholded)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing images in folder:  ./010725_mito_OA\\3K_no_OA_settings\n",
      "Analyzing images in folder:  ./010725_mito_OA\\3K_OA_settings\n",
      "Analyzing images in folder:  ./010725_mito_OA\\wt_settings\n",
      "Analyzing images in folder:  ./011425_mito_OA\\1K\n",
      "Analyzing images in folder:  ./011425_mito_OA\\3K_no_OA\n",
      "Analyzing images in folder:  ./011425_mito_OA\\3k_OA\n",
      "Analyzing images in folder:  ./011425_mito_OA\\WT\n",
      "Analyzing images in folder:  ./012125_mito_OA\\1K\n",
      "Analyzing images in folder:  ./012125_mito_OA\\3K_no_OA\n",
      "Analyzing images in folder:  ./012125_mito_OA\\3K_OA\n",
      "Analyzing images in folder:  ./012125_mito_OA\\WT\n",
      "Analyzing images in folder:  ./012825_mito_OA\\1K\n",
      "Analyzing images in folder:  ./012825_mito_OA\\3K_no_OA\n",
      "Analyzing images in folder:  ./012825_mito_OA\\3K_OA\n",
      "Analyzing images in folder:  ./012825_mito_OA\\WT\n",
      "Analyzing images in folder:  ./020425_mito_OA\\1K\n",
      "Analyzing images in folder:  ./020425_mito_OA\\3K_no_OA\n",
      "Analyzing images in folder:  ./020425_mito_OA\\3K_OA\n",
      "Analyzing images in folder:  ./020425_mito_OA\\WT\n",
      "Analyzing images in folder:  ./021125_mito_OA\\1K\n",
      "Analyzing images in folder:  ./021125_mito_OA\\3K_no_OA\n",
      "Analyzing images in folder:  ./021125_mito_OA\\3K_OA\n",
      "Analyzing images in folder:  ./021125_mito_OA\\WT\n",
      "Analysis Complete\n"
     ]
    }
   ],
   "source": [
    "analyze_main_folder('./',False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e5f431",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246cba3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab12780",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
